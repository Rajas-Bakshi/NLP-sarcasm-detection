{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\n",
      "/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", lines =1)\n",
    "for i in range(0, 26709):\n",
    "    df.iloc[i,1] = df.iloc[i,1].replace (\"'\",'')\n",
    "    \n",
    "X = df.iloc[:,1]\n",
    "y= df.iloc[:,2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "for i in range (0,26709):\n",
    "    sent = re.sub(\"[^A-Za-z]\", \" \", X[i])\n",
    "    sent = sent.lower().split()\n",
    "    sent = [ps.stem(word) for word in sent if word not in set(stopwords.words(\"english\"))]\n",
    "    sent = \" \".join(sent)\n",
    "    corpus.append(sent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1000)\n",
    "X_processed = cv.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test =train_test_split(X_processed, y, test_size = 0.2, random_state  = 10, shuffle = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear SVC classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "simple_classifier = LinearSVC()\n",
    "simple_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2523  562]\n",
      " [ 508 1749]]\n",
      "0.7997004867090978\n"
     ]
    }
   ],
   "source": [
    "y_linearsvc_pred = simple_classifier.predict(X_test)\n",
    "print(confusion_matrix (y_linearsvc_pred,y_test))\n",
    "print(accuracy_score (y_linearsvc_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=1000, units=128, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=2, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21367/21367 [==============================] - 2s 88us/step - loss: 0.6359 - accuracy: 0.7139\n",
      "Epoch 2/10\n",
      "21367/21367 [==============================] - 1s 35us/step - loss: 0.5245 - accuracy: 0.8129\n",
      "Epoch 3/10\n",
      "21367/21367 [==============================] - 1s 34us/step - loss: 0.4835 - accuracy: 0.8226\n",
      "Epoch 4/10\n",
      "21367/21367 [==============================] - 1s 40us/step - loss: 0.4580 - accuracy: 0.8286\n",
      "Epoch 5/10\n",
      "21367/21367 [==============================] - 1s 35us/step - loss: 0.4403 - accuracy: 0.8327\n",
      "Epoch 6/10\n",
      "21367/21367 [==============================] - 1s 34us/step - loss: 0.4268 - accuracy: 0.8328\n",
      "Epoch 7/10\n",
      "21367/21367 [==============================] - 1s 34us/step - loss: 0.4158 - accuracy: 0.8345\n",
      "Epoch 8/10\n",
      "21367/21367 [==============================] - 1s 34us/step - loss: 0.4068 - accuracy: 0.8364\n",
      "Epoch 9/10\n",
      "21367/21367 [==============================] - 1s 35us/step - loss: 0.3991 - accuracy: 0.8370\n",
      "Epoch 10/10\n",
      "21367/21367 [==============================] - 1s 33us/step - loss: 0.3928 - accuracy: 0.8385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff63024feb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_classifier = Sequential()\n",
    "ann_classifier.add(Dense(output_dim = 128, init ='uniform', activation = 'relu', input_dim = 1000 ))\n",
    "ann_classifier.add(Dense(output_dim = 2, init ='uniform', activation = 'relu'))\n",
    "ann_classifier.add(Dense(output_dim =1, init ='uniform', activation ='sigmoid'))\n",
    "\n",
    "ann_classifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "ann_classifier.fit(X_train,y_train, batch_size = 100, nb_epoch= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2562  577]\n",
      " [ 469 1734]]\n",
      "0.804193186072632\n"
     ]
    }
   ],
   "source": [
    "ann_y_pred = ann_classifier.predict(X_test)\n",
    "ann_y_pred = (ann_y_pred > 0.5)\n",
    "\n",
    "print(confusion_matrix(ann_y_pred,y_test))\n",
    "print(accuracy_score(ann_y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
